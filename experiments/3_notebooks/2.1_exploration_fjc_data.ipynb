{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview <a id=ov>\n",
    "\n",
    "1. [Features](#features)\n",
    "2. [Label Construction and Analysis](#labels)\n",
    "3. [Feature Analysis (Mini)](#feature_ana)\n",
    "4. [Model Definition and Training](#model)\n",
    "5. [Training Visualization](#train_vis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from os import listdir\n",
    "from copy import deepcopy\n",
    "from os.path import isdir\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "from itertools import cycle\n",
    "from utils import retrieve_if_necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_experiment_descriptor_fjc(fname):\n",
    "    segs = fname.split('__')\n",
    "    instance_name = segs[2].split('_')[0]\n",
    "    seed = segs[2].split('_')[1][len('seeded'):]\n",
    "    # print(instance_name, seed)\n",
    "    return instance_name, seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "csv_list = []\n",
    "\n",
    "for fname in listdir(data_path):\n",
    "    fp = f'{data_path}{fname}'\n",
    "    if not isdir(fp):\n",
    "        partial_df = pd.read_csv(fp, index_col=0)\n",
    "        ename, seed = get_experiment_descriptor_fjc(fname)\n",
    "        partial_df['benchmark_name'] = [ename] * partial_df.shape[0]\n",
    "        partial_df['seed'] = int(seed)\n",
    "        csv_list.append(partial_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "df_complete = pd.concat(csv_list)\n",
    "print(df_complete.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seeds = np.unique(df_complete['seed'])\n",
    "\n",
    "seeds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append('../../../fabricatio-controls/')\n",
    "\n",
    "from fabricatio_controls.comparison_utils import store_seeds\n",
    "\n",
    "seed_fp = f'../1_data/seeds/1_analysis_seeds_{setup}.txt'\n",
    "\n",
    "store_seeds(seeds, seed_fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Features <a id=features>\n",
    "    \n",
    "[Back to Overview](#ov)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Somewhat too many columns. 400 out of them are the raw state inputs which we are not so interested in. These are prefixed with `op_t_`, `op_d_`, `op_l_`, `op_s_` for the type, location, duration and status matrices respectively. \n",
    "\n",
    "Let us drop these for a sec."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "column_subset = [c for c in df_complete.columns \n",
    "                 if c[:5] not in ['op_t_', 'op_d_', 'op_l_', 'op_s_']]\n",
    "\n",
    "\n",
    "df_subset1 = df_complete[column_subset]\n",
    "\n",
    "unique_vals = df_subset1.nunique()\n",
    "\n",
    "pd.set_option('display.max_rows', unique_vals.shape[0]+1)\n",
    "display(unique_vals)\n",
    "print(len(unique_vals))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of these remaining 111 columns, the ones starting with a heuristic name (e.g. `SRPT_`) followed by `NoneType_` and a KPI mark the achieved respective goal by the particular jeuristics.\n",
    "\n",
    "Available goals are, as you can see, `makespan`, `tardiness`, `utilization`, `throughput_op` (i.e. operation throughput), `throughput_j` (i.e. job throughput) and `flow_time`. We will stick with the makespan for now. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cumulative_r_names = [\n",
    "    'cum_r_util_ave_diff_continuous', 'cum_r_util_ave_diff_discrete',\n",
    "    'cum_r_util_exp', 'cum_r_util_timescaled', 'cum_r_util_std_diff_discrete',\n",
    "    'cum_r_makespan_continuous', 'cum_r_makespan_normed', 'cum_r_buff_len'\n",
    "]\n",
    "\n",
    "heuristic_names = [\n",
    "    'SPT_LQT', 'LPT_LQT', 'LOR_LQT', 'MOR_LQT', \n",
    "    'SRPT_LQT', 'LRPT_LQT', 'LTPO_LQT', \n",
    "    'MTPO_LQT', 'EDD_LQT', 'LUDM_LQT']\n",
    "\n",
    "col_subset2 = [c for c in df_subset1.columns \n",
    "               if ('_'.join(c.split('_')[:2]) not in heuristic_names \n",
    "               or (c.split('_')[-1] == 'makespan' or c.split('_')[2] == 'cum')) \n",
    "               or (c.split('_')[0] in heuristic_names \n",
    "               and c.split('_')[1] != 'NoneType')]\n",
    "\n",
    "df_subset2 = df_complete[col_subset2]\n",
    "\n",
    "unique_vals = df_subset2.nunique()\n",
    "\n",
    "pd.set_option('display.max_rows', unique_vals.shape[0]+1)\n",
    "display(unique_vals)\n",
    "print(len(unique_vals))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice :)\n",
    "    \n",
    "Now, from these columns, `direct_action` and ``indirect_action`` cannot be used to predict the best heuristic/the makespan achieved by the best heuristic since these represent the decisions taken by the winning heuristic, i.e. they are basically the label. The dicrete field `benchmark_name` is also somethingour model cannot know beforehand. Hence we exclude these three columns as well.\n",
    "\n",
    "\n",
    "**If you're wondering about the fields marked with the `r_` prefix, these are possible quantities used for *rewards* during the training of an RL scheduler and could very well be used as features, since they are available at every decision point. Note however, that the column should be shifted appropriately, since the reward $r_{t}$ is available only after a decision has been made given the state $s_{t}$. As such, for every partial dataframe, the reward columns should be shifted by one, such that $s_{t}$ is associated with $s_{t-1}$.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "col_subset3 = [c for c in df_subset2.columns \n",
    "               if c not in [\"direct_action\", \"indirect_action\", \"benchmark_name\"]]\n",
    "\n",
    "df_subset3 = df_subset2[col_subset3]\n",
    "\n",
    "unique_vals = df_subset3.nunique()\n",
    "\n",
    "pd.set_option('display.max_rows', unique_vals.shape[0]+1)\n",
    "display(unique_vals)\n",
    "print(len(unique_vals))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "f_name = 'wip_to_arrival_ratio'\n",
    "\n",
    "def mean_std(group_df, f_name):\n",
    "    d = {}\n",
    "    d[f'{f_name}_ave'] = group_df[f_name].mean()\n",
    "    d[f'{f_name}_std'] = group_df[f_name].std()\n",
    "    return pd.Series(d)\n",
    "\n",
    "def get_step_plot_df(df_complete, f_name):\n",
    "    return (df_complete[[f_name, 'n_steps']]\n",
    "            .groupby('n_steps').apply(lambda x: mean_std(x, f_name) )\n",
    "            .reset_index())\n",
    "\n",
    "def get_time_plot_df(df_complete, f_name):\n",
    "    \"\"\"\n",
    "    Creates the mean and standard deviation signals for the argument feature at the \n",
    "    distinct action times. To speed up the plotting process, the signals are resampled \n",
    "    every 100 time units and aggreggated using the mean.\n",
    "    \"\"\"\n",
    "    df_feature = df_complete[['t_action', f_name]]\n",
    "    line_rgr_vis_df = df_feature.sort_values('t_action').reset_index(drop=True)\n",
    "    line_rgr_vis_df.index = pd.to_datetime(line_rgr_vis_df.index)\n",
    "    std = line_rgr_vis_df.resample('100ns').std()\n",
    "    means = line_rgr_vis_df.resample('100ns').mean()\n",
    "    df_plot = means.copy()\n",
    "    df_plot = df_plot.rename(columns={f_name: f'{f_name}_ave'})\n",
    "    df_plot[f'{f_name}_std'] = std[f_name]\n",
    "    return df_plot\n",
    "    \n",
    "df_plot = get_step_plot_df(df_complete, f_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import itertools\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "sns.set_context('talk')\n",
    "pal = itertools.cycle(sns.color_palette('cubehelix', 2))\n",
    "\n",
    "def plot_feature_distribution(f_name, df_plot, ylabel='', x_axis='n_steps', x_label=\"Step\"):\n",
    "    _, ax = plt.subplots(1, 1, figsize=(5.5, 3.75))#, gridspec_kw={'height_ratios': [3, 1]})\n",
    "    \n",
    "    ax = sns.lineplot(data=df_plot, x=x_axis, y=f'{f_name}_ave', \n",
    "                 color=next(pal), ax=ax)\n",
    "    ax.fill_between(df_plot[x_axis],\n",
    "                     df_plot[f'{f_name}_ave'] - df_plot[f'{f_name}_std'], \n",
    "                     df_plot[f'{f_name}_ave'] + df_plot[f'{f_name}_std'], \n",
    "                     alpha=.5, color=next(pal))\n",
    "    ax.set_xlabel(x_label)\n",
    "    ax.set_ylabel(ylabel)\n",
    "#     ax = sns.histplot(x=df_plot[f'{f_name}_ave'], kde=True, ax=ax[1])\n",
    "#     ax.set_xlabel(ylabel)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{plot_base}/51_fjc__{f_name}.png\", dpi=200, bbox_inches='tight', pad_inches=0.02)\n",
    "    plt.show()\n",
    "\n",
    "# df_plot = get_step_plot_df(df_complete, 'wip_to_arrival_ratio')\n",
    "# plot_feature_distribution('wip_to_arrival_ratio', df_plot, \"WIP Fill\")\n",
    "# df_plot = get_step_plot_df(df_complete, 'utl_avg')\n",
    "# plot_feature_distribution('utl_avg', df_plot, \"Average System \\nUtilization\")\n",
    "df_plot = get_time_plot_df(df_complete, 'wip_to_arrival_ratio')\n",
    "plot_feature_distribution('wip_to_arrival_ratio', df_plot, x_axis=\"t_action\", x_label=\"Time\")\n",
    "df_plot = get_time_plot_df(df_complete, 'utl_avg')\n",
    "plot_feature_distribution('utl_avg', df_plot, x_axis=\"t_action\", x_label=\"Time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "f_names = [\n",
    "    'buffer_length_ratio',                        \n",
    "    'buffer_load_ratios_avg',                    \n",
    "    'buffer_load_ratios_std',                    \n",
    "    'buffer_time_ratio',                        \n",
    "    'decision_skip_ratio',\n",
    "    'duration_ave',                              \n",
    "    'duration_distance_mean',                    \n",
    "    'duration_distance_std',                    \n",
    "    'duration_entropy',                          \n",
    "    'duration_std',        \n",
    "    'estimated_flow_time_ave',\n",
    "    'estimated_flow_time_std',\n",
    "    'estimated_tardiness_rate',                   \n",
    "    'heuristic_agreement_entropy',                 \n",
    "    'job_op_completion_rate_ave',                 \n",
    "    'job_op_completion_rate_std',                 \n",
    "    'job_op_max_rel_completion_rate_avg',         \n",
    "    'job_op_max_rel_completion_rate_std',         \n",
    "    'job_work_completion_rate_avg',              \n",
    "    'job_work_completion_rate_std',              \n",
    "    'job_work_max_rel_completion_rate_avg',      \n",
    "    'job_work_max_rel_completion_rate_std',     \n",
    "    'kendalltau_ave',                            \n",
    "    'kendalltau_std',                            \n",
    "    'legal_action_job_ratio',                       \n",
    "    'legal_action_len_stream_ave',             \n",
    "    'legal_action_len_stream_std',             \n",
    "    'makespan_lb_ub_ratio',                    \n",
    "    'op_completion_rate',                         \n",
    "    'silhouette_kmeans_max_k',                    \n",
    "    'silhouette_kmeans_mid_k',                   \n",
    "    'silhouette_kmeans_min_k',                  \n",
    "    'tardiness_rate',                              \n",
    "    'throughput_time_j_abs_avg',                \n",
    "    'throughput_time_j_abs_std',               \n",
    "    'throughput_time_j_rel_avg',                \n",
    "    'throughput_time_j_rel_std',                \n",
    "    'type_ave',                                   \n",
    "    'type_entropy',                              \n",
    "    'type_hamming_mean',                         \n",
    "    'type_hamming_std',                          \n",
    "    'type_std',  \n",
    "    'utl_current',\n",
    "    'utl_avg',                                 \n",
    "    'utl_std',                                 \n",
    "    'wip_rel_sys_t',                           \n",
    "    'wip_to_arrival_ratio',                         \n",
    "    'wip_to_arrival_time_ratio',                  \n",
    "    'work_completion_rate',    \n",
    "]\n",
    "len(f_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('fea_names.txt', 'w') as f:\n",
    "    json.dump(f_names, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%time\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "sns.set_context('talk')\n",
    "\n",
    "\n",
    "def check_name_correspondence(fea_name_rank, df_tab):\n",
    "    # preprocessing\n",
    "    fea_name_rank = fea_name_rank.replace('utl', 'utilization')\n",
    "    fea_name_rank = fea_name_rank.replace('std', 'standard_deviation')\n",
    "    fea_name_rank = fea_name_rank.replace('avg', 'average')\n",
    "    fea_name_rank = fea_name_rank.replace('_j_', '_job_')\n",
    "    fea_name_rank = fea_name_rank.replace('job_work_max_rel', 'work_max_rel')\n",
    "    fea_name_rank = fea_name_rank.replace('rel', 'relative')\n",
    "    fea_name_rank = fea_name_rank.replace('abs', 'absolute')\n",
    "    fea_name_rank = fea_name_rank.replace('len', 'length')\n",
    "    # rank components\n",
    "    scores = []\n",
    "    names = []\n",
    "    for name in df_tab['Name']:\n",
    "        scores.append(0)\n",
    "        names.append(name)\n",
    "        searchstring = ''.join(name.lower().split()).replace('\\\\', '')\n",
    "        segs = fea_name_rank.split('_')\n",
    "        for seg in segs:\n",
    "            if seg in searchstring: \n",
    "                scores[-1] += 1\n",
    "    #print(scores)\n",
    "    best_score_pos = np.argmax(scores)\n",
    "    return names[best_score_pos], best_score_pos\n",
    "\n",
    "\n",
    "def add_figure_head():\n",
    "    tex_string = r'\\begin{figure}[ht!]' + '\\n'\n",
    "    tex_string += r'\\centering' + '\\n'\n",
    "    tex_string += r'\\resizebox{\\linewidth}{!}{' + '\\n'\n",
    "    tex_string += r'\\begin{tabular}{@{}ll@{}}' + '\\n'\n",
    "    return tex_string\n",
    "\n",
    "\n",
    "def add_figure_tail(setup_str, part):\n",
    "    plt_title = setup_str + 'Feature Behavior (' + part + ').'\n",
    "    tex_string = r'\\end{tabular}}' + '\\n'\n",
    "    tex_string += (r'\\caption[' + plt_title + r']{\\small ' + plt_title +\n",
    "                   r'}\\label{fig:ex_instances}' + '\\n')\n",
    "    tex_string += r'\\end{figure}' + '\\n\\n'\n",
    "    return tex_string\n",
    "\n",
    "def get_tex_string(f_name, f_display_name, i, setup='fjc'):\n",
    "    tex_string = ''\n",
    "    setup_string = r'$FJc$' if setup == 'fjc' else r'$Jm$'\n",
    "    if (i % 8 == 0):\n",
    "        if i != 0:\n",
    "            tex_string += add_figure_tail(setup_string, f'{i-7}-{i}')                \n",
    "        tex_string += add_figure_head()\n",
    "    subfloat_caption = (f'\\\\subfloat[\\n\\t\\\\scriptsize {setup_string} '\n",
    "                        f'{f_display_name} Behavior.')\n",
    "    label = '\\\\label{subfig:' + setup + '_' + f_name + '}\\n]'\n",
    "    includegraphics = '{\\n\\t\\\\includegraphics[width=0.45\\\\linewidth]{'\n",
    "    graphics_path = r'main_matter/images/x_' + setup + r'_' + f_name + r'.png}}'\n",
    "    line_ending = '&\\n' if i % 2 == 0 else '\\\\\\\\\\n'\n",
    "    return (tex_string + subfloat_caption + label + includegraphics + graphics_path + line_ending)\n",
    "\n",
    "stp = 'fjc'\n",
    "setup_string = r'$FJc$' if stp == 'fjc' else r'$Jm$'\n",
    "\n",
    "df_tab = pd.read_csv('fea_tab.csv')\n",
    "pal = itertools.cycle(sns.color_palette(\n",
    "    'cubehelix', int(1.5 * len(f_names))))\n",
    "\n",
    "tex_string = ''\n",
    "i = 0\n",
    "for f_name in f_names:\n",
    "    print(f_name)\n",
    "    if f_name == 'utl_avg':\n",
    "        continue\n",
    "    df_plot = get_time_plot_df(df_complete, f_name)\n",
    "    col = next(pal)\n",
    "    ax = sns.lineplot(data=df_plot, x='t_action', y=f'{f_name}_ave', color=col)\n",
    "    ax.fill_between(df_plot['t_action'], df_plot[f'{f_name}_ave'] - df_plot[f'{f_name}_std'], df_plot[f'{f_name}_ave'] + df_plot[f'{f_name}_std'], alpha=.5, color=col)\n",
    "    ax.set_xlabel('Simulation Time')\n",
    "    display_name, idx = check_name_correspondence(f_name, df_tab)\n",
    "    print(display_name)\n",
    "    ax.set_ylabel('')\n",
    "    tex_string += get_tex_string(f_name, display_name, i, stp)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'D://1_research//2_disertation//1_document//main_matter//images//x_{stp}_{f_name}.png', dpi=200, bbox_inches='tight', pad_inches=0.02)\n",
    "    plt.show()\n",
    "    i += 1\n",
    "\n",
    "tex_string += add_figure_tail(setup_string, f'{(i // 8) * 8 + 1}-{i}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(tex_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "n_cols = 4\n",
    "n_rows = int(np.ceil(len(f_names) / n_cols))\n",
    "\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(7 * n_cols, 10 * n_rows))#, \n",
    "                     #gridspec_kw={'height_ratios': [3, 1]})\n",
    "pal = itertools.cycle(sns.color_palette('cubehelix', int(1.5 * len(f_names))))\n",
    "\n",
    "i = 0\n",
    "for f_name in f_names:\n",
    "    df_plot = get_step_plot_df(df_complete, f_name) \n",
    "    col = next(pal)\n",
    "    ax = sns.lineplot(data=df_plot, x='n_steps', y=f'{f_name}_ave', \n",
    "             color=col, ax=axes[i // 4][i % 4])\n",
    "    ax.fill_between(df_plot['n_steps'],\n",
    "                    df_plot[f'{f_name}_ave'] - df_plot[f'{f_name}_std'], \n",
    "                    df_plot[f'{f_name}_ave'] + df_plot[f'{f_name}_std'], \n",
    "                    alpha=.5, color=col)\n",
    "    if i < (n_rows - 1) * n_cols - (n_cols - n_rows % 4):\n",
    "        ax.set_xticklabels([])\n",
    "        ax.set_xlabel('')\n",
    "    else:\n",
    "        ax.set_xlabel('Simulation Step')\n",
    "    ax.set_ylabel(' '.join([x.capitalize() for  x in f_name.split('_')]))\n",
    "    # ax[1] = sns.histplot(x=df_plot[f'{f_name}_ave'], kde=True, ax=ax[1])\n",
    "    i += 1\n",
    "    \n",
    "\n",
    "while i < n_rows * n_cols:\n",
    "    fig.delaxes(axes[i // 4][i % 4])\n",
    "    i += 1\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.subplots_adjust(hspace=0.2)\n",
    "\n",
    "plt.savefig(f'feature_overview_lineplots_{setup}.png', dpi=200)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "n_rows = int(np.ceil(len(f_names) / 4))\n",
    "\n",
    "fig, axes = plt.subplots(n_rows, 4, figsize=(28, 10 * n_rows))#, \n",
    "                     #gridspec_kw={'height_ratios': [3, 1]})\n",
    "    \n",
    "pal = itertools.cycle(sns.color_palette('cubehelix', int(1.5 * len(f_names))))\n",
    "\n",
    "i = 0\n",
    "for f_name in f_names:\n",
    "    df_plot = get_step_plot_df(df_complete, f_name) \n",
    "    col = next(pal)\n",
    "    ax = sns.histplot(x=df_plot[f'{f_name}_ave'].astype('float32'), kde=True, \n",
    "                      ax=axes[i // 4][i % 4], color=col)\n",
    "    if i // 4 < 10:\n",
    "        ax.set_xlabel('')\n",
    "    ax.set_ylabel(' '.join([x.capitalize() for  x in f_name.split('_')]))\n",
    "    \n",
    "    i += 1\n",
    "\n",
    "fig.delaxes(axes[i // 4][i % 4])\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'feature_overview_histplots_{setup}.png', dpi=200)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label Construction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us construct the classification/regression labels. For now we focus on AlphaZero style predictions meaning that we try and predict the state value in the case of regression and not the action value. For DQN style predictions we would need to do a simultaneous regression for all possible heuristics in the current state. A sicussion for another time ;)\n",
    "\n",
    "For classification, we simply select the heuristic having achieved the best makespan for any given state. For regression, we select the makespan score corresponding to the latter.\n",
    "\n",
    "To compute the labels, we have to: \n",
    "1. select the label fields\n",
    "2. get the argmin of the row for the clf label\n",
    "3. get the corresponding makespan value for the rgr label\n",
    "\n",
    "This will take a few seconds given the 800k+ datapoints..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subset3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_result_cols = [c for c in df_subset3.columns if c.endswith('makespan')]\n",
    "h_result_cnames = [c.split('_')[0] for c in h_result_cols]\n",
    "h_control_complete_names = ['_'.join(c.split('_')[:2]) for c in h_result_cols]\n",
    "\n",
    "cr_cols = []\n",
    "\n",
    "#list1_permutations = itertools.permutations(list1, len(list2))\n",
    "for cname in h_control_complete_names:\n",
    "    for rname in cumulative_r_names:\n",
    "        cr_cols.append(f'{cname}_{rname}')\n",
    "\n",
    "# display(all_combinations)\n",
    "# df_subset3[~df_subset3.isin(f_names)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in h_result_cols:\n",
    "    try:\n",
    "        assert (df_subset3['t_action'] > df_subset3[col]).sum() == 0\n",
    "    except AssertionError:\n",
    "        print(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# 1. select label fields\n",
    "df_label_raw = df_subset3[h_result_cols + cr_cols]\n",
    "# 2. get the best heuristic and its value\n",
    "def get_labels(row, clf_names, h_control_complete_names):\n",
    "    clf_label_encoded = np.argmin(row[h_result_cols])\n",
    "    clf_label_name = clf_names[clf_label_encoded]\n",
    "    pertinent_cumulative_reward_cols = [\n",
    "        f'{h_control_complete_names[clf_label_encoded]}_{cr}' \n",
    "        for cr in cumulative_r_names]\n",
    "    rgr_label = row[clf_label_encoded]\n",
    "    # print(row[pertinent_cumulative_reward_cols])\n",
    "    filtered_row = tuple([clf_label_encoded, clf_label_name, rgr_label]\n",
    "                         + row[pertinent_cumulative_reward_cols].tolist())\n",
    "    return filtered_row\n",
    "\n",
    "label_series = df_label_raw.apply(\n",
    "    lambda x: get_labels(x, h_result_cnames, h_control_complete_names), axis=1)\n",
    "label_df = pd.DataFrame(\n",
    "    label_series.values.tolist(), \n",
    "    columns=['clf_label', 'clf_label_name', 'rgr_label'] + cumulative_r_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_num_cols(df):\n",
    "    for col in df.columns:\n",
    "        if df[col].dtypes == 'float64':\n",
    "            df[col] = df[col].astype('float32')\n",
    "        elif df[col].dtypes == 'int64':\n",
    "            df[col] = df[col].astype('int32')\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "# test_df = label_df.copy()\n",
    "# print(test_df.dtypes)\n",
    "# convert_num_cols(test_df)\n",
    "# print(test_df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make best makespan relative by subtracting wip start time then replace the heuristic result fields with the newly constructed labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_factor_df = df_complete[[c for c in df_complete.columns if c[:5] == 'op_d_']]\n",
    "norm_factor = norm_factor_df.sum(axis=1)\n",
    "# norm_factor_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subset3['t_action']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_df['rgr_s_label'] = (label_df['rgr_label'] - df_subset3['t_action'].values) \n",
    "label_df['rgr_rel_label'] = (label_df['rgr_s_label'] \n",
    "                             / norm_factor.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# label_df.index = df_subset3.index\n",
    "\n",
    "label_df = label_df.reset_index(drop=True)\n",
    "df_final = df_subset3.drop(h_result_cols + cr_cols, axis=1).reset_index(drop=True)\n",
    "convert_num_cols(label_df)\n",
    "convert_num_cols(df_final)\n",
    "\n",
    "for col in label_df.columns:\n",
    "    df_final[col] = label_df[col]\n",
    "\n",
    "unique_vals = df_final.nunique()\n",
    "\n",
    "pd.set_option('display.max_rows', unique_vals.shape[0]+1)\n",
    "\n",
    "display(unique_vals)\n",
    "print(len(unique_vals))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the features here are nummeric or ordinal so you're basically good to go :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Label Construction and Visualization  <a id=labels>\n",
    "\n",
    "[Back to Overview](#ov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import cycle\n",
    "\n",
    "sns.set_context('talk')\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "def plot_heuristic_comparison(clf_label_list, figname=\"\"):\n",
    "    # TODO: consistent colors between heuristic names and \n",
    "    # corresponding bars between plots ;)\n",
    "    palette = cycle(sns.color_palette('cubehelix', 2))\n",
    "    names, values = np.unique(clf_label_list,return_counts=True)\n",
    "    values = values / values.sum()\n",
    "    green = next(palette)\n",
    "    magenta = next(palette)\n",
    "    colors = [green if values[i] > 0.05 else magenta\n",
    "           for i in range(values.shape[0])]\n",
    "    ax = sns.barplot(x=names, y=values, palette=colors)\n",
    "    ax.set_xticklabels(names, rotation=45)\n",
    "    # fig = plt.figure(figsize=(15, 8))\n",
    "    # ax = sns.histplot(sorted(df_final['clf_label_name']), \n",
    "    #                   palette='cubehelix', discrete=True)\n",
    "    #ax.set_xticklabels(h_result_cnames)\n",
    "    ax.set_xlabel(\"Heuristic Name\")\n",
    "    ax.set_ylabel(\"Success Rate\")\n",
    "    plt.tight_layout()\n",
    "    if figname:\n",
    "        plt.savefig(f'{plot_base}/{figname}.png', \n",
    "                    dpi=200, bbox_inches='tight', pad_inches=0.02)\n",
    "\n",
    "plot_heuristic_comparison(sorted(df_final['clf_label_name']), \n",
    "                          f\"52_heuristic_distribution_{setup}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_p_df(df_final):\n",
    "    el, counts = np.unique(df_final['clf_label'], return_counts=True)\n",
    "    counts_list = []       \n",
    "    ps = counts / counts.sum()\n",
    "    ave = int(counts.mean())\n",
    "    subsamples = []\n",
    "    for i in range(10):\n",
    "        if i in el:\n",
    "            idx = np.where(el == i)\n",
    "        if ps[idx] <= 0.05:\n",
    "            continue\n",
    "        h_sample = df_final[df_final['clf_label'] == i]\n",
    "        subsamples.append(h_sample)\n",
    "    df_resampled = pd.concat(subsamples, axis=0)\n",
    "    return df_resampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_mean_df(df_final):\n",
    "    el, counts = np.unique(df_final['clf_label'], return_counts=True)\n",
    "    ave = int(counts.mean())\n",
    "    subsamples = []\n",
    "    for i in range(10):\n",
    "        h_sample = df_final[df_final['clf_label'] == i]\n",
    "        subsamples.append(h_sample.sample(min(ave, h_sample.shape[0])))\n",
    "    df_resampled = pd.concat(subsamples, axis=0)\n",
    "    return df_resampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "df_resampled = filter_p_df(df_final)\n",
    "\n",
    "# fig = plt.figure(figsize=(15, 8))\n",
    "# ax = sns.histplot(sorted(df_resampled['clf_label_name']), \n",
    "#                   palette='cubehelix', discrete=True)\n",
    "\n",
    "plot_heuristic_comparison(sorted(df_resampled['clf_label_name']), \n",
    "                          f\"52_besth_distribution_{setup}\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[DEPRECATED DESCRIPTION]\n",
    "\n",
    "Before movin any further we need to take another look at the regression label, with which there is a problem: Because of the data gathering process, the makespan label shifted in time with each step. The makespan label always includes the time needed for job long gone fromm the wip, of wich our future model knows nearly nothing. We could assume, that using the field `t_start`, which indicates the system time at which the step was taken, a regression model could learn to approximate the correct heuristic results by simply adding this time to whatever prediction it would yiel. Nevertheless we can make the model's life easier by manually ajusting the regression labels relative to the step system time. \n",
    "\n",
    "We can then drop the baseline field. This has the added advantage that it makes heuristic results comparable over all the datapoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_resampled = df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_context('poster')\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "h_names = ['SPT', 'LPT', 'LOR', 'MOR', 'SRPT', 'LRPT', 'LTPO', 'MTPO', 'EDD', 'LUDM']\n",
    "fig = plt.figure(figsize=(15, 8))\n",
    "ax = sns.boxplot(x=df_resampled['clf_label'], y=df_resampled['rgr_label'], \n",
    "                 order=list(range(len(h_names))))\n",
    "ax.set_xticklabels(h_names)\n",
    "ax.set_title(\"Distribution of Heuristic Makespan in 'Win Cases'\", loc='left')\n",
    "ax.set_xlabel(\"Heuristic\")\n",
    "ax.set_ylabel(\"'Win' Makespan\") \n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{plot_base}/when_do_heuristics_win_{setup}.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "line_rgr_vis_df = df_resampled.copy().sort_values('t_action').reset_index(drop=True)\n",
    "\n",
    "line_rgr_vis_df.index = pd.to_datetime(line_rgr_vis_df.index)\n",
    "std = line_rgr_vis_df.resample('100ns').std()\n",
    "means = line_rgr_vis_df.resample('100ns').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "fig = plt.figure(figsize=(15, 8))\n",
    "sns.lineplot(data=means, x='t_action', y='rgr_label')\n",
    "\n",
    "plt.fill_between(means['t_action'],\n",
    "                 means['rgr_label'] - std['rgr_label'], \n",
    "                 means['rgr_label'] + std['rgr_label'], alpha=.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_context('poster')\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "h_names = ['SPT', 'LPT', 'LOR', 'MOR', 'SRPT', 'LRPT', 'LTPO', 'MTPO', 'EDD', 'LUDM']\n",
    "fig = plt.figure(figsize=(15, 8))\n",
    "ax = sns.boxplot(x=df_resampled['clf_label'], y=df_resampled['rgr_s_label'], \n",
    "                 order=list(range(len(h_names))))\n",
    "ax.set_xticklabels(h_names)\n",
    "ax.set_title(\"Distribution of WIP Start Relative Heuristic Makespan in 'Win Cases'\", \n",
    "             loc='left')\n",
    "ax.set_xlabel(\"Heuristic\")\n",
    "ax.set_ylabel(\"'Win' Makespan\") \n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{plot_base}/when_do_heuristics_win.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (df_final['rgr_s_label'] > 0).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "fig = plt.figure(figsize=(15, 8))\n",
    "sns.lineplot(data=means, x='t_action', y='rgr_s_label')\n",
    "plt.fill_between(means['t_action'],\n",
    "                 means['rgr_s_label'] - std['rgr_s_label'], \n",
    "                 means['rgr_s_label'] + std['rgr_s_label'], alpha=.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_context('poster')\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "h_names = ['SPT', 'LPT', 'LOR', 'MOR', 'SRPT', 'LRPT', 'LTPO', 'MTPO', 'EDD', 'LUDM']\n",
    "fig = plt.figure(figsize=(15, 8))\n",
    "ax = sns.boxplot(x=df_resampled['clf_label'], y=df_resampled['rgr_rel_label'], \n",
    "                 order=list(range(len(h_names))))\n",
    "ax.set_xticklabels(h_names)\n",
    "ax.set_title(\"Distribution of Start and Duration Relative \"\n",
    "             \"Heuristic Makespan in 'Win Cases'\", loc='left')\n",
    "ax.set_xlabel(\"Heuristic\")\n",
    "ax.set_ylabel(\"'Win' Makespan\") \n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{plot_base}/when_do_heuristics_win.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "fig = plt.figure(figsize=(15, 8))\n",
    "sns.lineplot(data=means, x='t_action', y='rgr_rel_label')\n",
    "plt.fill_between(means['t_action'],\n",
    "                 means['rgr_rel_label'] - std['rgr_rel_label'], \n",
    "                 means['rgr_rel_label'] + std['rgr_rel_label'], alpha=.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resource Utilization Over Decision Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "fig = plt.figure(figsize=(15, 8))\n",
    "utl_ave = means['utl_avg'] \n",
    "sns.lineplot(x=means['t_action'], y=utl_ave)\n",
    "plt.fill_between(means['t_action'],\n",
    "                 means['utl_avg'] - std['utl_std'], \n",
    "                 means['utl_avg'] + std['utl_std'], alpha=.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictors = f_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_cols = [c for c in df_resampled.columns if c.startswith('r_')]\n",
    "\n",
    "groups = (df_final[r_cols + ['n_steps', 'seed']]\n",
    "          .sort_values(['seed', 'n_steps'])\n",
    "          .groupby(['seed'])\n",
    "          .cumsum())\n",
    "# groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_r_analysis_full = df_final.sort_values(['seed', 'n_steps'])\n",
    "df_r_analysis_full[r_cols] = groups[r_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_predictor_test = df_final.dropna()\n",
    "print(df_predictor_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_r_analysis_end = (df_r_analysis_full[df_r_analysis_full.columns]\n",
    "                     .sort_values(['seed', 'n_steps'])\n",
    "                     .groupby(['seed']).last())\n",
    "# df_r_analysis_end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Feature Analysis (Mini) <a id=feature_ana>\n",
    "\n",
    "[Back to Overview](#ov)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some correlations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# _ = plt.figure(figsize=(60, 30))\n",
    "\n",
    "# sns.heatmap(df_resampled.corr().abs(), \n",
    "#             linewidths=.5, annot=True, vmin=0.1, fmt='1.2f', \n",
    "#             vmax=1, \n",
    "#             cmap='cubehelix')\n",
    "# plt.tight_layout()\n",
    "# plt.savefig(f'feature_correlation_{setup}.png', dpi=300)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us take a cloaser look at the correlation between the cumulative reward and the regression labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_reward_correlations(df_cum_r, cr_cols, figname_suffix=''):\n",
    "    l_cols = [\n",
    "        'rgr_label', \n",
    "#         'rgr_s_label', \n",
    "#         'rgr_rel_label'\n",
    "    ]\n",
    "    df_r = df_cum_r[l_cols + cr_cols]\n",
    "    df_corr = df_r.corr()\n",
    "    names = {\n",
    "        \"rgr_label\": \"T: Makespan\",\n",
    "#         \"rgr_s_label\": \"T2: End-WIP \\nStart-Relative \\nMakespan\",\n",
    "#         \"rgr_rel_label\": \"T3: End-WIP \\nStart-Relative \\nNormed Makespan\",\n",
    "        \"cum_r_util_ave_diff_continuous\": \"R1: Utilization \\nContinuous Difference\",\n",
    "        \"cum_r_util_ave_diff_discrete\": \"R2: Utilization \\nQuantized Difference\",\n",
    "        \"cum_r_util_exp\": \"R3: Utilization \\nSigmoid\",\n",
    "        \"cum_r_util_timescaled\": \"R4: Timescaled \\nUtilization Difference\",\n",
    "        \"cum_r_util_std_diff_discrete\": \"R5: Utilization \\nQuantized Deviation\",\n",
    "        \"cum_r_makespan_continuous\": \"R6: Negative \\nAbsolute Makespan\",\n",
    "        \"cum_r_makespan_normed\": \"R7: Duration \\nRelative Makespan\",\n",
    "        \"cum_r_buff_len\": \"R8: Negative \\nCumulative Buffer Length\"\n",
    "    }\n",
    "    df_corr.rename(columns = names, index = names, inplace=True)\n",
    "    #df_corr.rename() = df_corr.columns.copy()\n",
    "    _ = plt.figure(figsize=(15, 13))\n",
    "\n",
    "    sns.set_context('poster')\n",
    "    sns.heatmap(df_corr, \n",
    "                linewidths=.5, annot=True, vmin=-1, fmt='1.2f', \n",
    "                vmax=1, \n",
    "                cmap='cubehelix')\n",
    "    plt.tight_layout()\n",
    "    if figname_suffix:\n",
    "        plt.savefig(f'{plot_base}/53_reward_correlation_{setup}_{figname_suffix}.png', dpi=300, \n",
    "                    bbox_inches='tight', pad_inches=0.02)\n",
    "    plt.show()\n",
    "    \n",
    "def get_reward_choice(df_cum_r, cr_cols, target_label, rank_position):\n",
    "    l_cols = [\n",
    "        'rgr_label', \n",
    "        'rgr_s_label', \n",
    "        'rgr_rel_label'\n",
    "    ]\n",
    "    df_r = df_cum_r[l_cols + cr_cols]\n",
    "    df_corr = df_r.corr()\n",
    "    df_corr.loc[target_label, \n",
    "                ['rgr_label', 'rgr_rel_label', 'rgr_s_label']] = [0, 0, 0]\n",
    "    return df_corr.columns[\n",
    "        np.where(\n",
    "            sorted(df_corr.loc[target_label,:])[rank_position - 1] \n",
    "            == df_corr.loc[target_label,:])].values[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The three regression labels are all important in different ways. The normed action time relative label emphasizes \"winns\" for individual wip configurations whitout considering by how much. The action time relative label emphasizes the magnitude of the makespan for individual wip windows. Last but not least, the \"raw\" regression label relativizes the curent success with respect to the past. \n",
    "\n",
    "Since we are dealing with an online problem, we want our reward estimate the overall makespan as well as possible. Hence, we choose the reward that best correlates with the overall makespan as our candidate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cr_cols = [f'cum_{c}' for c in r_cols]\n",
    "plot_reward_correlations(df_r_analysis_full, cr_cols, figname_suffix='full')\n",
    "\n",
    "print(get_reward_choice(df_r_analysis_full, cr_cols, 'rgr_label', 1))\n",
    "print(get_reward_choice(df_r_analysis_full, cr_cols, 'rgr_s_label', 1))\n",
    "print(get_reward_choice(df_r_analysis_full, cr_cols, 'rgr_rel_label', 1))\n",
    "\n",
    "plot_reward_correlations(df_r_analysis_end, cr_cols)\n",
    "print(get_reward_choice(df_r_analysis_end, cr_cols, 'rgr_label', 1))\n",
    "print(get_reward_choice(df_r_analysis_end, cr_cols, 'rgr_s_label', 1))\n",
    "print(get_reward_choice(df_r_analysis_end, cr_cols, 'rgr_rel_label', 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import cycle\n",
    "\n",
    "_, axes = plt.subplots(4, 2, figsize=(15, 13))\n",
    "\n",
    "\n",
    "palette = cycle(sns.color_palette('cubehelix', 12))\n",
    "for i in range(len(cr_cols)):\n",
    "    sns.histplot(df_r_analysis_full[cr_cols[i]], \n",
    "                 kde=True, ax=axes[i // 2][i % 2], \n",
    "                 color=next(palette), bins=30)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{plot_base}/reward_distributions.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_cols = [\n",
    "    'rgr_label', \n",
    "    'rgr_s_label', \n",
    "    'rgr_rel_label'\n",
    "]\n",
    "\n",
    "_, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "palette = cycle(sns.color_palette('cubehelix', 4))\n",
    "for i in range(len(l_cols)):\n",
    "    sns.histplot(df_r_analysis_full[l_cols[i]], \n",
    "                 kde=True, ax=axes[i % 3], color=next(palette), )\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{plot_base}/label_distributions.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_choice = get_reward_choice(df_r_analysis_full, cr_cols, 'rgr_label', 1)\n",
    "reward_choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_predictor_test = df_r_analysis_full.dropna()\n",
    "print(df_predictor_test.shape)\n",
    "fin_cpm = df_final.dropna()\n",
    "print(fin_cpm.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us see how well the selected reward can be predcted, and which features are most important :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.model_selection import cross_validate, train_test_split\n",
    "\n",
    "def get_feature_rankings(cv_results: dict, predictors: List[str], scoring=\"MAE\"):\n",
    "    \"\"\"\n",
    "    Extracts feature rankings from a cv results object containing a trained \n",
    "    random forest model.\n",
    "    \"\"\"\n",
    "    # get feature rankings\n",
    "    feature_scores = []\n",
    "    for estimator in cv_results['estimator']:\n",
    "        feature_scores.append(estimator.feature_importances_)\n",
    "\n",
    "    df_feature_importance = pd.DataFrame({\n",
    "        'feature_name': predictors,\n",
    "        'feature_importance': np.stack(feature_scores).mean(axis=0)\n",
    "    })\n",
    "    print(f\"Cross-Validated {scoring}: {cv_results['test_score'].mean()}\")\n",
    "    return df_feature_importance.sort_values(\n",
    "        'feature_importance', ascending=False).reset_index(drop=True)\n",
    "\n",
    "def train_random_forest_ranker(predictors: List[str], label: str, clf: bool = False):\n",
    "    seeds_tried = []\n",
    "    top10 = None\n",
    "    i = 1\n",
    "    while True:   \n",
    "        seed = 58629 # np.random.choice(90000, 1)[0]\n",
    "        print(seed)\n",
    "        seeds_tried.append(seed)\n",
    "        if clf==True:\n",
    "            feature_ranker = RandomForestClassifier()\n",
    "            scoring = 'accuracy'\n",
    "        else:\n",
    "            feature_ranker = RandomForestRegressor(random_state=seed)\n",
    "            scoring = \"neg_mean_absolute_error\"\n",
    "        df_predictor_test_sample = df_predictor_test.sample(10000, random_state=seed)\n",
    "        cv_results = cross_validate(feature_ranker, \n",
    "                                    df_predictor_test_sample[predictors + ['t_action']], \n",
    "                                    df_predictor_test_sample[label],\n",
    "                                    cv=10, scoring=scoring, \n",
    "                                    return_estimator=True, \n",
    "                                    return_train_score=True, n_jobs=4)\n",
    "        top10 = get_feature_rankings(cv_results, predictors + ['t_action'])['feature_name'].values[:11]\n",
    "        print(top10)\n",
    "        break\n",
    "    return cv_results\n",
    "\n",
    "# 81784\n",
    "# ['job_work_completion_rate_avg' 'kendalltau_ave' 'type_hamming_std'\n",
    "#  'duration_distance_std' 'job_work_completion_rate_std'\n",
    "#  'type_hamming_mean' 'duration_distance_mean' 'op_completion_rate'\n",
    "#  'job_op_completion_rate_ave' 'duration_ave']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "estimator_container_abs_l = train_random_forest_ranker(predictors, 'rgr_label')\n",
    "f_importance_abs_l_df = get_feature_rankings(estimator_container_abs_l, predictors + ['t_action'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_importance_abs_l_df = f_importance_abs_l_df[~(f_importance_abs_l_df['feature_name'] == 't_action')]\n",
    "f_importance_abs_l_df.to_csv('5_feature_importance_fjc.csv')\n",
    "f_importance_abs_l_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fi_jm = pd.read_csv('5_feature_importance_jm.csv', index_col=0)\n",
    "# fi_fjc = pd.read_csv('5_feature_importance_fjc.csv', index_col=0)\n",
    "\n",
    "\n",
    "# def get_rl_model_features_overlap(df_fi):\n",
    "#     model_features = {'legal_action_len_stream_ave', 'tardiness_rate',\n",
    "#                       'legal_action_len_stream_std', 'wip_rel_sys_t',\n",
    "#                       'decision_skip_ratio', 'type_hamming_mean',\n",
    "#                       'duration_distance_mean', 'wip_to_arrival_ratio',\n",
    "#                       'estimated_tardiness_rate', 'estimated_flow_time_ave'}\n",
    "#     overlap = model_features.intersection(df_fi['feature_name'].values[:10])\n",
    "#     return overlap\n",
    "\n",
    "\n",
    "# def compute_rank(fi_df):\n",
    "#     rank = 1\n",
    "#     score = 1\n",
    "#     fi_df['rank'] = np.zeros(fi_jm.shape[0])\n",
    "#     for i in range(1, fi_df.shape[0]):\n",
    "#         if fi_df['feature_importance'][i] < score:\n",
    "#             # print(score)\n",
    "#             fi_df['rank'][i] = rank\n",
    "#             rank += 1\n",
    "#             score = fi_df['feature_importance'][i]\n",
    "#         elif fi_df['feature_importance'][i] == score:\n",
    "#             fi_df['rank'][i] = rank\n",
    "#     return fi_df\n",
    "        \n",
    "# fi_jm = fi_jm.sort_values('feature_importance', ascending=False)\n",
    "# fi_jm = compute_rank(fi_jm)\n",
    "# fi_fjc = fi_fjc.sort_values('feature_importance', ascending=False)\n",
    "# fi_fjc = compute_rank(fi_fjc)\n",
    "\n",
    "\n",
    "# fi_overall = fi_fjc.join(fi_jm.set_index('feature_name'), \n",
    "#                          on='feature_name', how='inner', lsuffix='_fjc', rsuffix='_jm')\n",
    "# fi_ov_sorted = fi_overall.sort_values(\n",
    "#     'rank_fjc', ascending=True).reset_index(drop=True)\n",
    "\n",
    "# fi_ov_sorted\n",
    "\n",
    "# print(\"Jm&FJc Features Overlap: \")\n",
    "# overlap_fi_ov = get_rl_model_features_overlap(fi_ov_sorted)\n",
    "# print(len(overlap_fi_ov), overlap_fi_ov)\n",
    "\n",
    "# fi_ov_sorted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classification Results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "estimator_container_clf = train_random_forest_ranker(predictors, 'clf_label', clf=True)\n",
    "f_importance_clf_df = get_feature_rankings(estimator_container_clf, predictors)\n",
    "\n",
    "overlap_clf = get_rl_model_features_overlap(f_importance_clf_df)\n",
    "print(len(overlap_clf), overlap_clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Top 10 regression\")\n",
    "top10_regr_df = df_feature_importance_clf.sort_values(\n",
    "    'feature_importance', ascending=False)[:10]\n",
    "print(top10_regr_df.to_latex())\n",
    "display(top10_regr_df['feature_name'].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standard Deviation of Heuristic Decision Scores Over Decision Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subset3.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "avg_std_ddf = df_label_raw.copy()\n",
    "# get normed label variance per row\n",
    "# for col in avg_std_ddf.columns:\n",
    "#     avg_std_ddf[col] -= df_subset3['t_action'].values\n",
    "#     avg_std_ddf[col] /= norm_factor.values\n",
    "    \n",
    "avg_std_ddf['t_action'] = df_subset3['t_action'].values\n",
    "\n",
    "# thin data\n",
    "line_label_mstd_df = deepcopy(avg_std_ddf).sort_values(\n",
    "    't_action').reset_index(drop=True)\n",
    "\n",
    "line_label_mstd_df.index = pd.to_datetime(line_label_mstd_df.index)\n",
    "std = line_label_mstd_df.resample('100ns').std()\n",
    "means = line_label_mstd_df.resample('100ns').mean()\n",
    "\n",
    "# plot\n",
    "sns.set_context('poster')\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "_, axes = plt.subplots(2, 1, \n",
    "                       figsize=(15, 20), \n",
    "                       gridspec_kw={'height_ratios': [3, 1]})\n",
    "means_no_start = means.drop(columns=['t_action'])\n",
    "\n",
    "df_label_raw.std(axis=1)\n",
    "\n",
    "fig = plt.figure(figsize=(20, 8))\n",
    "sns.lineplot(data=means, \n",
    "             x='t_action', \n",
    "             y=means_no_start.mean(axis=1), ax=axes[0], \n",
    "             label='Average Decision Scores')\n",
    "\n",
    "axes[0].fill_between(\n",
    "    means['t_action'],\n",
    "    means_no_start.mean(axis=1) - means_no_start.std(axis=1), \n",
    "    means_no_start.mean(axis=1) + means_no_start.std(axis=1), \n",
    "    color='red',\n",
    "    alpha=.3, label='Standard Deviation')\n",
    "\n",
    "sns.lineplot(data=means, \n",
    "             x='t_action', \n",
    "             y=means_no_start.std(axis=1), ax=axes[1])\n",
    "\n",
    "axes[0].set_ylabel('Decision Score \\n'\n",
    "                   '(Makespan Relative to Duration Sum)')\n",
    "axes[0].set_xlabel('')\n",
    "axes[1].set_ylabel('Decision Score\\n Standard Deviation')\n",
    "axes[1].set_xlabel('State Time')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "237.333px",
    "width": "818.333px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
